基于最小二乘法的一元线性回归方程
---
####要求

-[ ] 关于房价的，只需要一个自变量一个因变量

-[ ] 用到最小二乘，但不是封装好的函数，有算法的具体实现

####原理

#####最小二乘法

我们以最简单的一元线性模型来解释最小二乘法。什么是一元线性模型呢？ 
监督学习中，如果预测的变量是离散的，我们称其为分类（如决策树，支持向量机等），如果预测的变量是连续的，我们称其为回归。回归分析中，如果只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析。对于二维空间线性是一条直线；对于三维空间线性是一个平面，对于多维空间线性是一个超平面。


对于一元线性回归模型, 假设从总体中获取了n组观察值（X1，Y1），（X2，Y2）， …，（Xn，Yn）。对于平面中的这n个点，可以使用无数条曲线来拟合。要求样本回归函数尽可能好地拟合这组值。综合起来看，这条直线处于样本数据的中心位置最合理。 选择最佳拟合曲线的标准可以确定为：使总的拟合误差（即总残差）达到最小。有以下三个标准可以选择：

        （1）用“残差和最小”确定直线位置是一个途径。但很快发现计算“残差和”存在相互抵消的问题。
        （2）用“残差绝对值和最小”确定直线位置也是一个途径。但绝对值的计算比较麻烦。
        （3）最小二乘法的原则是以“残差平方和最小”确定直线位置。用最小二乘法除了计算比较方便外，得到的估计量还具有优良特性。这种方法对异常值非常敏感。

最常用的是普通最小二乘法（Ordinary Least Square，OLS）：所选择的回归模型应该使所有观察值的残差平方和达到最小。（Q为残差平方和）- 即采用平方损失函数。

![image](https://github.com/TheSmurfs/Githubphotos/blob/master/%E6%96%87%E7%AB%A0%E9%85%8D%E5%9B%BE/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95.png?raw=true)